{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LATAM DE Challenge\n",
    "\n",
    "## Consideraciones:\n",
    "\n",
    "1. Dado el tamaño del dataset (no es grande) no se utilizará Spark debido a que no es necesario procesamiento distribuido.\n",
    "2. Se asume que el json entregado es correcto y no tiene errores en alguno de sus campos. Por ejemplo, se asume que los emoji se encuentran correctamente armados desde el dataset\n",
    "3. Para el desarrollo se emplea Google BigQuery. En este notebook no se encuentran las credenciales de servicio, tampoco en el código.\n",
    "4. Este notebook almacena los resultados de la ejecución de las funciones en BigQuery.\n",
    "5. Se creo un archivo Python para realizar benchmark del job en BigQuery. Esto es el archivo src/gcp_benchmark.py\n",
    "6. Considerando que toda la ejecución del job se realiza en BigQuery, no aporta realizar un benchmark de memoria y cpu de cada script. Puesto que estos se ejecutan de forma local y lo único que realizan es enviar la SQL query usando la API de Google BigQuery y recibir el resultado, no realizan ningun procesamiento. Por otro lado, el uso de memoria y CPU es constante en todos los casos puesto que el output entregado por BigQuery siempre son 10 registros de similar tamaño y tipo de datos, por lo que el uso de recursos computacionales es constante y no influye en la realización de este desafío. Finalmente, no se realizan mediciones locales (client Python), si no todo de GCP BigQuery (server GCP).\n",
    "\n",
    "## Forma de trabajo\n",
    "\n",
    "Se uso Gitflow para manejo de ramas: main - develop y luego de develop se crean ramas \"feature\" que son mergeadas a \"develop\" para luego realizar un \"release\" hacia main.\n",
    "\n",
    "## Estructura del notebook:\n",
    "\n",
    "Por cada caso (Qn) y su enfoque (memory o time) se presenta en orden:\n",
    "1. Consulta SQL realizada.\n",
    "2. Explicación.\n",
    "3. Tradeoffs.\n",
    "4. Mediciones y comparativas entre los enfoques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de-challenge-gm.tweets.farmers-protest-tweets\n"
     ]
    }
   ],
   "source": [
    "## Configuracion del proyecto\n",
    "# El dataset se encuentra cargado en una tabla en BigQuery\n",
    "# El filepath corresponde a la ubicación en BigQuery\n",
    "PROJECT_ID: str = \"de-challenge-gm\"\n",
    "DATASET_ID: str = \"tweets\"\n",
    "TABLE_NAME: str = \"farmers-protest-tweets\"\n",
    "\n",
    "file_path = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_NAME}\"\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q1: top 10 fechas donde hay más tweets\n",
    "\n",
    "## Q1 BigQuery SQL Query: MEMORY\n",
    "\n",
    "```python \n",
    "Q1_MEMORY_QUERY: str = \"\"\"\n",
    "-- CTE: contar tweets por fecha\n",
    "WITH date_counts AS (\n",
    "  SELECT\n",
    "    -- Convertir fecha a Date\n",
    "    DATE(date) AS tweet_date,\n",
    "    -- Contar numero de tweets por fecha\n",
    "    COUNT(*) AS tweet_count\n",
    "  FROM\n",
    "    `{file_path}`\n",
    "  GROUP BY\n",
    "    DATE(date)\n",
    "),\n",
    "-- CTE: seleccionar top10 fechas con mas tweets\n",
    "top_10_dates AS (\n",
    "  SELECT tweet_date, tweet_count\n",
    "  FROM date_counts\n",
    "  ORDER BY tweet_count DESC\n",
    "  LIMIT 10\n",
    "),\n",
    "-- CTE: contar tweets por user y fecha dentro de top10\n",
    "user_counts AS (\n",
    "  SELECT\n",
    "    DATE(date) AS tweet_date,\n",
    "    user.username,\n",
    "    -- Contar el numero de tweets por user en cada fecha\n",
    "    COUNT(*) AS user_tweet_count\n",
    "  FROM\n",
    "    `{file_path}`\n",
    "  WHERE DATE(date) IN (SELECT tweet_date FROM top_10_dates)\n",
    "  GROUP BY\n",
    "    DATE(date), user.username\n",
    ")\n",
    "-- Por cada fecha seleccionar la fecha y user con mas tweets\n",
    "SELECT\n",
    "  t.tweet_date,\n",
    "  -- Seleccionar el user con mas tweets en cada fecha\n",
    "  ARRAY_AGG(u.username ORDER BY u.user_tweet_count DESC LIMIT 1)[OFFSET(0)] AS top_user\n",
    "FROM\n",
    "  top_10_dates t\n",
    "JOIN\n",
    "  user_counts u\n",
    "ON\n",
    "  t.tweet_date = u.tweet_date\n",
    "GROUP BY\n",
    "  t.tweet_date, t.tweet_count\n",
    "-- Ordenar resultado de forma descedente\n",
    "ORDER BY\n",
    "  t.tweet_count DESC\n",
    "\"\"\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 20:12:30,303 - INFO - Starting: q1_memory\n",
      "2024-07-05 20:12:30,304 - INFO - Ejecutando query\n",
      "2024-07-05 20:12:32,674 - INFO - Tiempo de ejecución de la query (lado del cliente, python, hasta recibir la respuesta): 2.37 segundos\n",
      "2024-07-05 20:12:32,675 - INFO - Se uso query cache?: False\n",
      "2024-07-05 20:12:32,676 - INFO - Bigquery Job Detail:\n",
      "2024-07-05 20:12:32,676 - INFO - Job ID: 285537c8-bd20-4c14-933c-7a7b7ba0bb86\n",
      "2024-07-05 20:12:32,677 - INFO - Job Status: DONE\n",
      "2024-07-05 20:12:32,678 - INFO - Tiempo inicio en máquina GCP: 2024-07-06 00:12:31.422000+00:00\n",
      "2024-07-05 20:12:32,679 - INFO - Tiempo fin en máquina GCP: 2024-07-06 00:12:32.373000+00:00\n",
      "2024-07-05 20:12:32,679 - INFO - Tiempo de ejecución total en servidor GCP: 0.951 segundos\n",
      "2024-07-05 20:12:32,680 - INFO - Tiempo de ejecución total en cliente (python): 2.37 segundos\n",
      "2024-07-05 20:12:32,680 - INFO - Delta de tiempo (costo de red, serialización, SO, etc.): 1.42 segundos\n",
      "2024-07-05 20:12:32,681 - INFO - Bytes procesados: 2569405\n",
      "2024-07-05 20:12:32,681 - INFO - Bytes facturados: 10485760\n",
      "2024-07-05 20:12:32,682 - INFO - Slot machine miliseconds: 22696\n",
      "2024-07-05 20:12:32,682 - INFO - Uso estimado de memoria en base a total bytes procesados: 2.45 MB\n",
      "2024-07-05 20:12:32,682 - INFO - Query Plan:\n",
      "2024-07-05 20:12:32,683 - INFO -   Step S00: Input:\n",
      "2024-07-05 20:12:32,684 - INFO -     - Records read: 117407\n",
      "2024-07-05 20:12:32,684 - INFO -     - Records written: 13\n",
      "2024-07-05 20:12:32,684 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 20:12:32,685 - INFO -   Step S01: Sort+:\n",
      "2024-07-05 20:12:32,685 - INFO -     - Records read: 13\n",
      "2024-07-05 20:12:32,686 - INFO -     - Records written: 10\n",
      "2024-07-05 20:12:32,686 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 20:12:32,686 - INFO -   Step S02: Sort:\n",
      "2024-07-05 20:12:32,686 - INFO -     - Records read: 10\n",
      "2024-07-05 20:12:32,687 - INFO -     - Records written: 10\n",
      "2024-07-05 20:12:32,687 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 20:12:32,687 - INFO -   Step S03: Coalesce:\n",
      "2024-07-05 20:12:32,688 - INFO -     - Records read: 10\n",
      "2024-07-05 20:12:32,688 - INFO -     - Records written: 10\n",
      "2024-07-05 20:12:32,689 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 20:12:32,689 - INFO -   Step S04: Join+:\n",
      "2024-07-05 20:12:32,689 - INFO -     - Records read: 117537\n",
      "2024-07-05 20:12:32,690 - INFO -     - Records written: 44159\n",
      "2024-07-05 20:12:32,691 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 20:12:32,691 - INFO -   Step S05: Aggregate:\n",
      "2024-07-05 20:12:32,692 - INFO -     - Records read: 44159\n",
      "2024-07-05 20:12:32,693 - INFO -     - Records written: 44159\n",
      "2024-07-05 20:12:32,694 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 20:12:32,695 - INFO -   Step S07: Coalesce:\n",
      "2024-07-05 20:12:32,695 - INFO -     - Records read: 44159\n",
      "2024-07-05 20:12:32,695 - INFO -     - Records written: 44159\n",
      "2024-07-05 20:12:32,696 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 20:12:32,696 - INFO -   Step S08: Join+:\n",
      "2024-07-05 20:12:32,696 - INFO -     - Records read: 44169\n",
      "2024-07-05 20:12:32,696 - INFO -     - Records written: 10\n",
      "2024-07-05 20:12:32,697 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 20:12:32,697 - INFO -   Step S09: Aggregate+:\n",
      "2024-07-05 20:12:32,697 - INFO -     - Records read: 10\n",
      "2024-07-05 20:12:32,698 - INFO -     - Records written: 10\n",
      "2024-07-05 20:12:32,698 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 20:12:32,698 - INFO -   Step S0A: Output:\n",
      "2024-07-05 20:12:32,698 - INFO -     - Records read: 10\n",
      "2024-07-05 20:12:32,699 - INFO -     - Records written: 10\n",
      "2024-07-05 20:12:32,699 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 20:12:33,131 - INFO - Successful finish: q1_memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "# Ejecución q1_memory\n",
    "import importlib\n",
    "import q1_memory\n",
    "importlib.reload(q1_memory)\n",
    "result = q1_memory.q1_memory(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pasos q1_memory:\n",
    "1. Cuenta tweets por fecha: crea tabla temporal date_count, esto temporalmente aumenta el uso de la memoria pero posteriormente se libera. Luego cuenta número de tweets por cada fehca y agrupa los resultados por \"tweet_date\"\n",
    "2. Identifica top10 fechas con más tweets: crea tabla temporal \"top_10_dates\", en donde almacena la data de los tweets por fecha y count para luego limitarlo en 10 registros. Con esto es eficiente en memoria pero costoso en tiempo CPU.\n",
    "3. Dentro de estas 10 fechas, cuenta el número de tweets por usuario en cada fecha.\n",
    "4. Selecciona el usuario con mayor número de tweets para cada fecha. Se utiliza \"ARRAY_AGG\" el cuál se encuentra optimizado para agregaciones, en especifico, acá se agrega usernames ordenados por user_tweet_count de forma descendente.\n",
    "\n",
    "Para la construcción de la consulta previamente mostrada (q1_memory) se utilizaron los siguientes criterios para su construcción, considerando que la priorización es el uso de memoria, es decir, minimizar el uso de memoria:\n",
    "\n",
    "* Para reducir el consumo de memoria deberia ir filtrando (reduciendo) el espacio de búsqueda por cada step. De forma que cuando termina un step, se libera recursos y avanza con el nuevo conjunto. De esta forma el sistema operativo termianndo cada paso libera y no mantiene ocupada la memoria ram con la totalidad del dataset durante toda la ejecución del job. \n",
    "* Se utilizo Common Table Expressions (CTE) secuenciales para procesar los datos por etapas. Con esto sólo se mantiene los datos que se utilizan en memoria y por cada paso del query plan se liberan los recursos no utilizados. \n",
    "* La ventaja del enfoque anterior es que en efecto se usa menor recursos de memoria, porque cada step del query job sólo emplea en memoria los datos con los cuales trabajan, esto se evidencia en que cada Step S0X (01, 02, ... 0A) el número de registros leidos disminuye hasta ser constante. Sin embargo, esto es más lento, dado que es un procesamiento secuencial con múltiples etapas.\n",
    "* Desventaja del enfoque es que se realiza un full scan de la tabla.\n",
    "\n",
    "#### Análisis q1_memory\n",
    "\n",
    "* **uso de memoria**: eficiente. Progresivamente reduce el conjunto de datos residente en memoria por lo que libera recursos en cada step.\n",
    "* **tiempo de ejecución**: alto. Dado que existen múltiples etapas de procesamiento aumenta el tiempo de ejecución.\n",
    "* **uso de CPU**: intermedio. Requiere realizar un full scan de la tabla y realizar una agregación. Mientras más grande la tabla mayor tiempo de CPU, aunque esto va a depender si se realizo algun index. En este caso no se realizaron indexes ni se guardaron resultados de consultas en materialized views.\n",
    "* **uso de disco**: alto inicialmente. Comienza alto por la ingestion pero posteriormente por cada step del query plan se ve que la cantidad de registros leidos y escritos va disminuyendo. \n",
    "\n",
    "\n",
    "#### Posible mejora (q1_memory):\n",
    "* Paralizar los steps, sin embargo esto conlleva a un aumento de costos puesto que es mayor cantidad de nodos a utilizar.\n",
    "* Realizar un particionamiento o clustering por usuario. Actualmente la tabla cargada en BigQuery se encuentra particionada por la fecha (date)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 BigQuery SQL Query: TIME\n",
    "\n",
    "```python \n",
    "Q1_TIME_QUERY: str = \"\"\"\n",
    "-- Funcion para seleccionar el usuario con mas tweets en una fecha\n",
    "CREATE TEMP FUNCTION TopUserForDate(date_users ARRAY<STRUCT<username STRING, count INT64>>)\n",
    "RETURNS STRING\n",
    "LANGUAGE js AS '''\n",
    "  return date_users.reduce((a, b) => a.count > b.count ? a : b).username;\n",
    "''';\n",
    "\n",
    "-- CTE: contar tweets por user y fecha\n",
    "WITH date_user_counts AS (\n",
    "  SELECT\n",
    "    -- Convertir fecha a Date\n",
    "    DATE(date) AS tweet_date,\n",
    "    user.username,\n",
    "    -- Contar el numero de tweets por user en cada fecha\n",
    "    COUNT(*) AS tweet_count\n",
    "  FROM\n",
    "    `{file_path}`\n",
    "  GROUP BY\n",
    "    DATE(date), user.username\n",
    "),\n",
    "-- CTE: agregar totales de tweets por fecha\n",
    "--      y seleccionar users con mas tweets\n",
    "date_totals AS (\n",
    "  SELECT\n",
    "    tweet_date,\n",
    "    -- Sumar numero total de tweets en cada fecha\n",
    "    SUM(tweet_count) AS total_count,\n",
    "    -- Crear array de users con sus count correspondiente de tweets\n",
    "    ARRAY_AGG(STRUCT(username, tweet_count AS count) ORDER BY tweet_count DESC LIMIT 1) AS top_users\n",
    "  FROM\n",
    "    date_user_counts\n",
    "  GROUP BY\n",
    "    tweet_date\n",
    ")\n",
    "-- Por cada fecha seleccionar la fecha y user con mas tweets\n",
    "SELECT\n",
    "  tweet_date,\n",
    "  -- Uso de la funcion para obtener el user con mas tweets en cada fecha\n",
    "  TopUserForDate(top_users) AS top_user\n",
    "FROM (\n",
    "  SELECT *\n",
    "  FROM date_totals\n",
    "  ORDER BY total_count DESC\n",
    "  LIMIT 10\n",
    ")\n",
    "-- Order total de tweets descendente\n",
    "ORDER BY\n",
    "  total_count DESC\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 14:17:17,526 - INFO - Starting: q1_time\n",
      "2024-07-05 14:17:17,526 - INFO - Ejecutando query\n",
      "2024-07-05 14:17:19,151 - INFO - Tiempo de ejecución de la query (lado del cliente, python, hasta recibir la respuesta): 1.62 segundos\n",
      "2024-07-05 14:17:19,152 - INFO - Se uso query cache?: False\n",
      "2024-07-05 14:17:19,153 - INFO - Bigquery Job Detail:\n",
      "2024-07-05 14:17:19,153 - INFO - Job ID: 80b46508-6a65-4ee4-9ecb-63c551d06751\n",
      "2024-07-05 14:17:19,154 - INFO - Job Status: DONE\n",
      "2024-07-05 14:17:19,154 - INFO - Tiempo inicio en máquina GCP: 2024-07-05 18:17:18.112000+00:00\n",
      "2024-07-05 14:17:19,155 - INFO - Tiempo fin en máquina GCP: 2024-07-05 18:17:18.789000+00:00\n",
      "2024-07-05 14:17:19,155 - INFO - Tiempo de ejecución total en servidor GCP: 0.677 segundos\n",
      "2024-07-05 14:17:19,156 - INFO - Tiempo de ejecución total en cliente (python): 1.62 segundos\n",
      "2024-07-05 14:17:19,157 - INFO - Delta de tiempo (costo de red, serialización, SO, etc.): 0.95 segundos\n",
      "2024-07-05 14:17:19,157 - INFO - Bytes procesados: 2569405\n",
      "2024-07-05 14:17:19,158 - INFO - Bytes facturados: 10485760\n",
      "2024-07-05 14:17:19,159 - INFO - Slot machine miliseconds: 1890\n",
      "2024-07-05 14:17:19,159 - INFO - Uso estimado de memoria en base a total bytes procesados: 2.45 MB\n",
      "2024-07-05 14:17:19,160 - INFO - Query Plan:\n",
      "2024-07-05 14:17:19,160 - INFO -   Step S00: Input:\n",
      "2024-07-05 14:17:19,161 - INFO -     - Records read: 117407\n",
      "2024-07-05 14:17:19,161 - INFO -     - Records written: 51646\n",
      "2024-07-05 14:17:19,161 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 14:17:19,162 - INFO -   Step S01: Aggregate+:\n",
      "2024-07-05 14:17:19,162 - INFO -     - Records read: 51646\n",
      "2024-07-05 14:17:19,163 - INFO -     - Records written: 13\n",
      "2024-07-05 14:17:19,163 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 14:17:19,163 - INFO -   Step S02: Sort+:\n",
      "2024-07-05 14:17:19,164 - INFO -     - Records read: 13\n",
      "2024-07-05 14:17:19,164 - INFO -     - Records written: 10\n",
      "2024-07-05 14:17:19,164 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 14:17:19,165 - INFO -   Step S03: Sort+:\n",
      "2024-07-05 14:17:19,165 - INFO -     - Records read: 10\n",
      "2024-07-05 14:17:19,166 - INFO -     - Records written: 10\n",
      "2024-07-05 14:17:19,166 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 14:17:19,167 - INFO -   Step S04: Output:\n",
      "2024-07-05 14:17:19,167 - INFO -     - Records read: 10\n",
      "2024-07-05 14:17:19,168 - INFO -     - Records written: 10\n",
      "2024-07-05 14:17:19,169 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 14:17:19,489 - INFO - Successful finish: q1_time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(datetime.date(2021, 2, 12), 'RanbirS00614606'), (datetime.date(2021, 2, 13), 'MaanDee08215437'), (datetime.date(2021, 2, 17), 'RaaJVinderkaur'), (datetime.date(2021, 2, 16), 'jot__b'), (datetime.date(2021, 2, 14), 'rebelpacifist'), (datetime.date(2021, 2, 18), 'neetuanjle_nitu'), (datetime.date(2021, 2, 15), 'jot__b'), (datetime.date(2021, 2, 20), 'MangalJ23056160'), (datetime.date(2021, 2, 23), 'Surrypuria'), (datetime.date(2021, 2, 19), 'Preetm91')]\n"
     ]
    }
   ],
   "source": [
    "# Ejecución q1_time\n",
    "import q1_time\n",
    "import importlib\n",
    "importlib.reload(q1_time)\n",
    "result = q1_time.q1_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pasos q1_time:\n",
    "\n",
    "A diferencia del enfoque anterior, acá se implemento una función en Javascript (UDF) para selecionar el usuario con más tweets en una fecha.\n",
    "1. Crea una función para seleccionar el usuario con más tweets en una fecha. Esto se realizo mediante un UDF debido a que personalmente me acomoda más escribir ese tipo de consultas mediante una sintaxis de paradigma funcional la cuál me provee Javascript (es un reduce), por otro lado, reduce realizar consultas SQLs complejas.\n",
    "2. Cuenta el número de tweets por usuario y fecha.\n",
    "3. Suma los totales de tweets por fecha y selecciona los usuarios con más tweets.\n",
    "4. Usa la función previamente definida y almacenada en memoria para obtener de forma funcional (salida solo depende de las entradas) al usuario con más tweets en cada una de las 10 fechas.\n",
    "\n",
    "Comparando con el enfoque q1_memory, el uso de la UDF simplifica y acelera la selección del usuario con más tweets. Esto reduce la cantidad de CTEs a almacenar puesto que este recurso de CTEs no se encuentra optimizadas para minimizar el volumen en las primeras etapas del procesamiento.\n",
    "\n",
    "Luego, la agrupación y sumas ocurren en una sola operación, reduciendo el número de steps en la query plan. Finalmente, limita y ordena los resultados antes de procesar más datos, reduciendo el uso de CPU en las siguientes consultas.\n",
    "\n",
    "#### Análisis q1_time\n",
    "\n",
    "* **uso de memoria**: eficiente. Al limitar los datos a 10 fechas en las primeras fases del query plan, limita el uso de datos residentes en memoria.\n",
    "* **tiempo de ejecución**: bajo. Al usar la UDF se optimiza el tiempo de uso de la CPU, es reducido comparado con el enfoque previo. Esto también ayuda a evitar múltiples etapas de procesamiento.\n",
    "* **uso de CPU**: intermedio. Si bien la query realiza un full scan de la tabla y multiples agregacias, el uso de CPU no se dispara debido a que todo se realiza en una sóla ejecución, por lo que el tiempo de uso CPU constante durante toda la operación.\n",
    "* **uso de disco**: alto inicialmente. Comienza alto por la ingestion pero posteriormente por cada step del query plan se ve que la cantidad de registros leidos y escritos va disminuyendo. \n",
    "\n",
    "\n",
    "#### Posible mejora (q1_time):\n",
    "* Paralizar los steps, sin embargo esto conlleva a un aumento de costos puesto que es mayor cantidad de nodos a utilizar.\n",
    "* Realizar un particionamiento o clustering por usuario. Actualmente la tabla cargada en BigQuery se encuentra particionada por la fecha (date).\n",
    "* Usar precomputacion o cache de agregaciones.\n",
    "* Usar indices en date y user.username. No obstante, la primera query debe crear esto para que las siguientes n+1 consultas puedan observar este beneficio.\n",
    "* Usar SQL nativo si se maneja las optimizaciones de BigQuery. En mi caso desconozco optimizaciones a nivel de SQL con BigQuery."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla Comparativa de Mediciones Q1\n",
    "\n",
    "### Configuración del job Q1\n",
    "\n",
    "| Parámetros                                      | q1_memory | q1_time |\n",
    "|------------------------------------------------|-----------|---------|\n",
    "| QUERY_CACHE                                    | False     | False   |\n",
    "| QUERY_PRIORITY                                 | BATCH     | INTERACTIVE |\n",
    "\n",
    "### Resultados mediciones\n",
    "\n",
    "| Medicion                                      | q1_memory | q1_time |\n",
    "|-----------------------------------------------|-----------|---------|\n",
    "| Tiempo ejecución en cliente                   | 2.37 s    | 1.62 s  |\n",
    "| Tiempo ejecución en GCP (server)              | 0.951 s   | 0.677 s |\n",
    "| Delta tiempo (client-server)                  | 1.42 s    | 0.95 s  |\n",
    "| Memoria usada por cliente python              | NA        | NA      |\n",
    "| CPU usado por cliente python                  | NA        | NA      |\n",
    "| Bytes Procesados en GCP (server)              | 2.45 MB   | 2.45 MB |\n",
    "| Bytes Facturados en GCP (server)              | 10.49 MB  | 10.49 MB|\n",
    "| Slot Machine tiempo usado (GCP CPU Time) (ms) | 22696 ms  | 1890 ms |\n",
    "| BigQuery Total steps en QueryPlan             | 11        | 5       |\n",
    "\n",
    "### Resultados Query Plan\n",
    "\n",
    "#### Query Plan: q1_memory\n",
    "\n",
    "| Step  | Descripción         | Records Read | Records Written | Status   |\n",
    "|-------|----------------------|--------------|-----------------|----------|\n",
    "| S00   | Input                | 117407       | 13              | COMPLETE |\n",
    "| S01   | Sort+                | 13           | 10              | COMPLETE |\n",
    "| S02   | Sort                 | 10           | 10              | COMPLETE |\n",
    "| S03   | Coalesce             | 10           | 10              | COMPLETE |\n",
    "| S04   | Join+                | 117537       | 44159           | COMPLETE |\n",
    "| S06   | Sort                 | 10           | 10              | COMPLETE |\n",
    "| S07   | Coalesce             | 10           | 10              | COMPLETE |\n",
    "| S08   | Join+                | 44169        | 10              | COMPLETE |\n",
    "| S09   | Aggregate+           | 10           | 10              | COMPLETE |\n",
    "| S0A   | Output               | 10           | 10              | COMPLETE |\n",
    "\n",
    "#### Query Plan: q1_time\n",
    "\n",
    "| Step  | Descripción         | Records Read | Records Written | Status   |\n",
    "|-------|----------------------|--------------|-----------------|----------|\n",
    "| S00   | Input                | 117407       | 51646           | COMPLETE |\n",
    "| S01   | Aggregate+           | 51646        | 13              | COMPLETE |\n",
    "| S02   | Sort+                | 13           | 10              | COMPLETE |\n",
    "| S03   | Sort+                | 10           | 10              | COMPLETE |\n",
    "| S04   | Output               | 10           | 10              | COMPLETE |\n",
    "\n",
    "### Comparación Query Plan enfoque memory vs time\n",
    "\n",
    "| Step | q1_memory (Q1_MEMORY_QUERY)                                   | q1_time (Q1_TIME_QUERY)                                    |\n",
    "|------|---------------------------------------------------------------|-----------------------------------------------------------|\n",
    "| S00  | **Input**: Records read: 117407, Records written: 13, Status: COMPLETE | **Input**: Records read: 117407, Records written: 51646, Status: COMPLETE |\n",
    "| S01  | **Sort+**: Records read: 13, Records written: 10, Status: COMPLETE | **Aggregate+**: Records read: 51646, Records written: 13, Status: COMPLETE |\n",
    "| S02  | **Sort**: Records read: 10, Records written: 10, Status: COMPLETE | **Sort+**: Records read: 13, Records written: 10, Status: COMPLETE |\n",
    "| S03  | **Coalesce**: Records read: 10, Records written: 10, Status: COMPLETE | **Sort+**: Records read: 10, Records written: 10, Status: COMPLETE |\n",
    "| S04  | **Join+**: Records read: 117537, Records written: 44159, Status: COMPLETE | **Output**: Records read: 10, Records written: 10, Status: COMPLETE |\n",
    "| S05  | **Aggregate**: Records read: 44159, Records written: 44159, Status: COMPLETE |                                                           |\n",
    "| S07  | **Coalesce**: Records read: 44159, Records written: 44159, Status: COMPLETE |                                                           |\n",
    "| S08  | **Join+**: Records read: 44169, Records written: 10, Status: COMPLETE |                                                           |\n",
    "| S09  | **Aggregate+**: Records read: 10, Records written: 10, Status: COMPLETE |                                                           |\n",
    "| S0A  | **Output**: Records read: 10, Records written: 10, Status: COMPLETE |                                                           |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de los resultados anteriores se evidencia de que q1_time tiene una mejora con respecto al tiempo de ejecución versus su contraparte q1_memory.\n",
    "Esto se evidencia en los query plan de cada enfoque. Mientras que q1_memory tiene 10 pasos, q1_time tiene 4 (considerando output). En donde q1_memory va leyendo de a poco la data en cada paso, optimizando el uso de memoria durante la ejecución. Mientras que la version enfocada en el tiempo de ejecución se centra en obtener todo rápido (algo como un algoritmo voraz) realiza toda la ingesta y procesa todo sin depender de steps intermedios. La desventaja de esto es que el consumo de memoria y cpu es elevado. Sin embargo, la medición muestra que la máquina optimizo el uso de CPU y paso menos tiempo en CPU (dado la cantidad de steps).\n",
    "Se destaca que sólo para este paso se uso una configuración de job queue Batch para verificar como funcionaba el queue en batch para el dataset actual. El resto de casos se ocupo en Interactive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n",
    "\n",
    "* Para resolver Q2 se empleo rangos de regex para UNICODE. Acá hay una gran oportunidad de mejora, faltan rangos de unicode para emoticons por lo que es altamente probable que no capture todos los emoticons que existen dentro del dataset que se encuentra cargado en BigQuery.\n",
    "* Sin embargo como se presenta a continuación, el código es mantenible en este apartado puesto que para agregar más rangos no es necesario cambiar la consulta (tanto de q2_time o q2_memory) si no que simplemente se agrega en la constante UNICODE_RANGES.\n",
    "\n",
    "## Q2: SQL MEMORY\n",
    "\n",
    "```python \n",
    "# Unicode ranges\n",
    "EMOTICONS = r\"\\x{1F600}-\\x{1F64F}\"\n",
    "MISC_SYMBOLS_PICTOGRAPHS = r\"\\x{1F300}-\\x{1F5FF}\"\n",
    "TRANSPORT_MAP_SYMBOLS = r\"\\x{1F680}-\\x{1F6FF}\"\n",
    "FLAGS = r\"\\x{1F1E6}-\\x{1F1FF}\"\n",
    "MISC_SYMBOLS = r\"\\x{2600}-\\x{26FF}\"\n",
    "DINGBATS = r\"\\x{2700}-\\x{27BF}\"\n",
    "SUPPLEMENTAL_SYMBOLS_PICTOGRAPHS = r\"\\x{1F900}-\\x{1F9FF}\"\n",
    "SYMBOLS_PICTOGRAPHS_EXTENDED = r\"\\x{1FA70}-\\x{1FAFF}\"\n",
    "\n",
    "UNICODE_RANGES = (\n",
    "    f\"[{EMOTICONS}\"\n",
    "    f\"{MISC_SYMBOLS_PICTOGRAPHS}\"\n",
    "    f\"{TRANSPORT_MAP_SYMBOLS}\"\n",
    "    f\"{FLAGS}\"\n",
    "    f\"{MISC_SYMBOLS}\"\n",
    "    f\"{DINGBATS}\"\n",
    "    f\"{SUPPLEMENTAL_SYMBOLS_PICTOGRAPHS}\"\n",
    "    f\"{SYMBOLS_PICTOGRAPHS_EXTENDED}]\"\n",
    ")\n",
    "\n",
    "Q2_MEMORY_QUERY: str = rf\"\"\"\n",
    "-- Funcion: extraer emojis únicos de un string\n",
    "CREATE TEMP FUNCTION ExtractEmoji(content STRING) AS (\n",
    "  -- usar ARRAY_AGG con DISTINCT para eliminar duplicados de inmediato\n",
    "  (SELECT ARRAY_AGG(DISTINCT char IGNORE NULLS)\n",
    "   -- Separa content en char\n",
    "   FROM UNNEST(SPLIT(content, '')) AS char\n",
    "   -- Filtra caracteres que hacen match con rangos unicode\n",
    "   WHERE REGEXP_CONTAINS(char, r'{UNICODE_RANGES}'))\n",
    ");\n",
    "\n",
    "-- Main Query\n",
    "WITH emoji_counts AS (\n",
    "  -- Extrae y cuenta emojis en una pasada\n",
    "  SELECT\n",
    "    emoji,\n",
    "    COUNT(*) as count\n",
    "  FROM\n",
    "     `{{file_path}}`,\n",
    "    -- Usa la función para extraer emojis y hace unnest de los resultados\n",
    "    UNNEST(ExtractEmoji(content)) as emoji\n",
    "  -- Group por emoji unico\n",
    "  GROUP BY\n",
    "    emoji\n",
    ")\n",
    "-- Select the top 10 emojis\n",
    "SELECT emoji, count\n",
    "FROM (\n",
    "  SELECT\n",
    "    emoji,\n",
    "    count,\n",
    "    -- Asigna un ranking cada emoji basado en su count y desc\n",
    "    RANK() OVER (ORDER BY count DESC) as rank\n",
    "  FROM emoji_counts\n",
    ")\n",
    "WHERE rank <= 10\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 19:55:54,180 - INFO - Starting: q2_memory\n",
      "2024-07-05 19:55:54,180 - INFO - Ejecutando query\n",
      "2024-07-05 19:55:56,127 - INFO - Tiempo de ejecución de la query (lado del cliente, python, hasta recibir la respuesta): 1.95 segundos\n",
      "2024-07-05 19:55:56,129 - INFO - Se uso query cache?: False\n",
      "2024-07-05 19:55:56,129 - INFO - Bigquery Job Detail:\n",
      "2024-07-05 19:55:56,130 - INFO - Job ID: 2314a991-5332-4252-ac8d-a5c8b0cf9a7a\n",
      "2024-07-05 19:55:56,131 - INFO - Job Status: DONE\n",
      "2024-07-05 19:55:56,131 - INFO - Tiempo inicio en máquina GCP: 2024-07-05 23:55:54.612000+00:00\n",
      "2024-07-05 19:55:56,132 - INFO - Tiempo fin en máquina GCP: 2024-07-05 23:55:55.682000+00:00\n",
      "2024-07-05 19:55:56,132 - INFO - Tiempo de ejecución total en servidor GCP: 1.07 segundos\n",
      "2024-07-05 19:55:56,133 - INFO - Tiempo de ejecución total en cliente (python): 1.95 segundos\n",
      "2024-07-05 19:55:56,134 - INFO - Delta de tiempo (costo de red, serialización, SO, etc.): 0.88 segundos\n",
      "2024-07-05 19:55:56,134 - INFO - Bytes procesados: 21869484\n",
      "2024-07-05 19:55:56,135 - INFO - Bytes facturados: 22020096\n",
      "2024-07-05 19:55:56,135 - INFO - Slot machine miliseconds: 28137\n",
      "2024-07-05 19:55:56,136 - INFO - Uso estimado de memoria en base a total bytes procesados: 20.86 MB\n",
      "2024-07-05 19:55:56,136 - INFO - Query Plan:\n",
      "2024-07-05 19:55:56,136 - INFO -   Step S00: Input:\n",
      "2024-07-05 19:55:56,137 - INFO -     - Records read: 117407\n",
      "2024-07-05 19:55:56,138 - INFO -     - Records written: 361\n",
      "2024-07-05 19:55:56,139 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:55:56,139 - INFO -   Step S01: Aggregate:\n",
      "2024-07-05 19:55:56,139 - INFO -     - Records read: 361\n",
      "2024-07-05 19:55:56,140 - INFO -     - Records written: 70\n",
      "2024-07-05 19:55:56,140 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:55:56,141 - INFO -   Step S02: Aggregate+:\n",
      "2024-07-05 19:55:56,141 - INFO -     - Records read: 70\n",
      "2024-07-05 19:55:56,141 - INFO -     - Records written: 1\n",
      "2024-07-05 19:55:56,142 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:55:56,143 - INFO -   Step S03: Aggregate:\n",
      "2024-07-05 19:55:56,143 - INFO -     - Records read: 1\n",
      "2024-07-05 19:55:56,144 - INFO -     - Records written: 1\n",
      "2024-07-05 19:55:56,144 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:55:56,145 - INFO -   Step S04: Sort+:\n",
      "2024-07-05 19:55:56,145 - INFO -     - Records read: 71\n",
      "2024-07-05 19:55:56,146 - INFO -     - Records written: 70\n",
      "2024-07-05 19:55:56,146 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:55:56,146 - INFO -   Step S05: Aggregate:\n",
      "2024-07-05 19:55:56,146 - INFO -     - Records read: 70\n",
      "2024-07-05 19:55:56,147 - INFO -     - Records written: 68\n",
      "2024-07-05 19:55:56,147 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:55:56,147 - INFO -   Step S06: Aggregate+:\n",
      "2024-07-05 19:55:56,148 - INFO -     - Records read: 68\n",
      "2024-07-05 19:55:56,148 - INFO -     - Records written: 32\n",
      "2024-07-05 19:55:56,149 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:55:56,149 - INFO -   Step S08: Coalesce:\n",
      "2024-07-05 19:55:56,149 - INFO -     - Records read: 32\n",
      "2024-07-05 19:55:56,150 - INFO -     - Records written: 32\n",
      "2024-07-05 19:55:56,150 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:55:56,150 - INFO -   Step S09: Join+:\n",
      "2024-07-05 19:55:56,151 - INFO -     - Records read: 1670\n",
      "2024-07-05 19:55:56,151 - INFO -     - Records written: 10\n",
      "2024-07-05 19:55:56,151 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:55:56,152 - INFO -   Step S0A: Output:\n",
      "2024-07-05 19:55:56,152 - INFO -     - Records read: 10\n",
      "2024-07-05 19:55:56,152 - INFO -     - Records written: 10\n",
      "2024-07-05 19:55:56,153 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:55:56,474 - INFO - Successful finish: q2_memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('✊', 1724), ('❤', 1471), ('✌', 202), ('☮', 170), ('♂', 139), ('♀', 113), ('✍', 91), ('♥', 56), ('⚔', 48), ('✅', 44)]\n"
     ]
    }
   ],
   "source": [
    "# Ejecución q2_memory\n",
    "import q2_memory\n",
    "import importlib\n",
    "importlib.reload(q2_memory)\n",
    "result = q2_memory.q2_memory(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pasos q2_memory:\n",
    "\n",
    "1. Se define función para extraer emojis únicos, esto es útil para los count.\n",
    "2. Contar los Emojis:\n",
    "  2.1 Define una Common Table Expression (CTE) llamada emoji_counts que:\n",
    "    Utiliza la función ExtractEmoji definido en el paso anterior para extraer emojis del contenido de los tweets.\n",
    "    Descompone los arrays de emojis en elementos individuales usando UNNEST.\n",
    "  2.2 Cuenta la frecuencia de aparición (COUNT) de cada emoji único.\n",
    "  2.3 Agrupa los emojis únicos y sus conteos correspondientes.\n",
    "3. Seleccionar los 10 Emojis Más Frecuentes:\n",
    "    3.1 Subquery: Asignar un ranking a cada emoji basado en su conteo. Luego se ordena de forma descedente. Esto se hace con la función \"RANK\"\n",
    "    3.2 Filtra los resultados para obtener sólo 10 emoji más frecuentes, es decir, ranking menor o igual a 10\n",
    "    3.3 Ordenar emojis\n",
    "\n",
    "#### Análisis q2_memory\n",
    "\n",
    "* **uso de memoria**: bajo. \"unnest\" ayuda a trabajar los datos en pequeñas listas reduciendo la cantidad de datos en memoria por cada query step.\n",
    "* **tiempo de ejecución**: intermedio. Las operaciones se realizan en una sola pasada/ejecución. Pero el tiempo puede variar considerando la cantidad de emojis por cada texto.\n",
    "* **uso de CPU**: alto. Las operaciones de \"unnest\" y \"regexp_contains\" son computacionalmente caras en términos de uso de CPU.\n",
    "* **uso de disco**: bajo. La query no requiere múltiples escrituras y lecturas de disco. Puesto que todo se realiza en memoria, es ecir, la extracción de los emojis y conteo se realiza en memoria, minimizando el uso de disco. La creación de tablas temporales y funciones BigQuery usa paginación para mantener la referencia \"fácil acceso\" en memoria principal por lo que la plataforma maneja estos aspectos de forma eficiente minimizando los accesos a disco.\n",
    "\n",
    "\n",
    "#### Posible mejora (q2_memory):\n",
    "* Clustering para agrupar campos de uso frecuente (ej: emojis frecuentes)\n",
    "* Indices para acelerar las operaciones de regex y split. No obstante, disminuyes tiempo de CPU pero aumentas uso de storage debido al tamaño de los indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: SQL TIME\n",
    "\n",
    "```python \n",
    "Q2_TIME_QUERY: str = rf\"\"\"\n",
    "CREATE TEMP FUNCTION ExtractEmoji(content STRING) AS (\n",
    "  (SELECT ARRAY_AGG(DISTINCT emoji IGNORE NULLS)\n",
    "   FROM UNNEST(REGEXP_EXTRACT_ALL(content, r'{UNICODE_RANGES}')) AS emoji)\n",
    ");\n",
    "\n",
    "SELECT\n",
    "  emoji,\n",
    "  COUNT(*) AS count\n",
    "FROM\n",
    "  `{{file_path}}`,\n",
    "  UNNEST(ExtractEmoji(content)) AS emoji\n",
    "GROUP BY\n",
    "  emoji\n",
    "ORDER BY\n",
    "  count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 19:56:05,260 - INFO - Starting: q2_time\n",
      "2024-07-05 19:56:05,261 - INFO - Ejecutando query\n",
      "2024-07-05 19:56:06,831 - INFO - Tiempo de ejecución de la query (lado del cliente, python, hasta recibir la respuesta): 1.57 segundos\n",
      "2024-07-05 19:56:06,832 - INFO - Se uso query cache?: False\n",
      "2024-07-05 19:56:06,832 - INFO - Bigquery Job Detail:\n",
      "2024-07-05 19:56:06,833 - INFO - Job ID: feba3c6f-55e7-4472-81d5-1dab16de334c\n",
      "2024-07-05 19:56:06,833 - INFO - Job Status: DONE\n",
      "2024-07-05 19:56:06,834 - INFO - Tiempo inicio en máquina GCP: 2024-07-05 23:56:05.682000+00:00\n",
      "2024-07-05 19:56:06,834 - INFO - Tiempo fin en máquina GCP: 2024-07-05 23:56:06.092000+00:00\n",
      "2024-07-05 19:56:06,835 - INFO - Tiempo de ejecución total en servidor GCP: 0.41 segundos\n",
      "2024-07-05 19:56:06,835 - INFO - Tiempo de ejecución total en cliente (python): 1.57 segundos\n",
      "2024-07-05 19:56:06,835 - INFO - Delta de tiempo (costo de red, serialización, SO, etc.): 1.16 segundos\n",
      "2024-07-05 19:56:06,836 - INFO - Bytes procesados: 21869484\n",
      "2024-07-05 19:56:06,837 - INFO - Bytes facturados: 22020096\n",
      "2024-07-05 19:56:06,838 - INFO - Slot machine miliseconds: 813\n",
      "2024-07-05 19:56:06,838 - INFO - Uso estimado de memoria en base a total bytes procesados: 20.86 MB\n",
      "2024-07-05 19:56:06,839 - INFO - Query Plan:\n",
      "2024-07-05 19:56:06,839 - INFO -   Step S00: Input:\n",
      "2024-07-05 19:56:06,840 - INFO -     - Records read: 117407\n",
      "2024-07-05 19:56:06,840 - INFO -     - Records written: 361\n",
      "2024-07-05 19:56:06,840 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:56:06,841 - INFO -   Step S01: Sort+:\n",
      "2024-07-05 19:56:06,841 - INFO -     - Records read: 361\n",
      "2024-07-05 19:56:06,842 - INFO -     - Records written: 10\n",
      "2024-07-05 19:56:06,842 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:56:06,842 - INFO -   Step S02: Output:\n",
      "2024-07-05 19:56:06,843 - INFO -     - Records read: 10\n",
      "2024-07-05 19:56:06,843 - INFO -     - Records written: 10\n",
      "2024-07-05 19:56:06,843 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 19:56:07,122 - INFO - Successful finish: q2_time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('✊', 1724), ('❤', 1471), ('✌', 202), ('☮', 170), ('♂', 139), ('♀', 113), ('✍', 91), ('♥', 56), ('⚔', 48), ('✅', 44)]\n"
     ]
    }
   ],
   "source": [
    "# Ejecución q2_time\n",
    "import q2_time\n",
    "import importlib\n",
    "importlib.reload(q2_time)\n",
    "result = q2_time.q2_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pasos q2_time:\n",
    "\n",
    "La principal diferencia con el enfoque anterior es que, ahora se define una función UDF igual que el q1_time para extraer emojis. Esta función \"ExtractEmoji\" utiliza \"REGEXP_EXTRACT_ALL\" que extrae todos los emoji de los contenidos de los tweets y agrega emoji únicos en un array usando \"ARRAY_GG\" con \"DISTINCT\"\n",
    "\n",
    "\n",
    "#### Análisis q2_time\n",
    "\n",
    "* **uso de memoria**: bajo. \"unnest\" ayuda a trabajar los datos en pequeñas listas reduciendo la cantidad de datos en memoria por cada query step.\n",
    "* **tiempo de ejecución**: bajo en comparación con el enfoque anterior. Las operaciones computacionalmente caras se realizan en una sola pasada/ejecución.\n",
    "* **uso de CPU**: alto. Las operaciones de \"unnest\" y \"regexp_contains\" son computacionalmente caras en términos de uso de CPU.\n",
    "* **uso de disco**: bajo. Idem que enfoque anterior.\n",
    "\n",
    "\n",
    "#### Posible mejora (q2_time):\n",
    "* Agrupar emojis frecuentes para mejorar futuras consultas. Una especie de cache. Almacenar precomputaciones.\n",
    "* Indices para acelerar instrucción de REGEX y split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla Comparativa de Mediciones Q2\n",
    "\n",
    "### Configuración del job Q2\n",
    "\n",
    "| Parámetros     | q2_memory | q2_time |\n",
    "|----------------|-----------|---------|\n",
    "| QUERY_CACHE    | False     | False   |\n",
    "| QUERY_PRIORITY | INTERACTIVE | INTERACTIVE |\n",
    "\n",
    "### Resultados mediciones\n",
    "\n",
    "| Medición                                      | q2_memory | q2_time |\n",
    "|-----------------------------------------------|-----------|---------|\n",
    "| Tiempo ejecución en cliente                   | 1.95 s    | 1.57 s  |\n",
    "| Tiempo ejecución en GCP (server)              | 1.07 s    | 0.41 s  |\n",
    "| Delta tiempo (client-server)                  | 0.88 s    | 1.16 s  |\n",
    "| Memoria usada por cliente python              | NA        | NA      |\n",
    "| CPU usado por cliente python                  | NA        | NA      |\n",
    "| Bytes Procesados en GCP (server)              | 20.86 MB  | 20.86 MB|\n",
    "| Bytes Facturados en GCP (server)              | 22.02 MB  | 22.02 MB|\n",
    "| Slot Machine tiempo usado (GCP CPU Time) (ms) | 28137 ms  | 813 ms  |\n",
    "| BigQuery Total steps en QueryPlan             | 10        | 3       |\n",
    "\n",
    "### Resultados Query Plan\n",
    "\n",
    "#### Query Plan: q2_memory\n",
    "\n",
    "| Step  | Descripción         | Records Read | Records Written | Status   |\n",
    "|-------|----------------------|--------------|-----------------|----------|\n",
    "| S00   | Input                | 117407       | 361             | COMPLETE |\n",
    "| S01   | Aggregate            | 361          | 70              | COMPLETE |\n",
    "| S02   | Aggregate+           | 70           | 1               | COMPLETE |\n",
    "| S03   | Aggregate            | 1            | 1               | COMPLETE |\n",
    "| S04   | Sort+                | 71           | 70              | COMPLETE |\n",
    "| S05   | Aggregate            | 70           | 68              | COMPLETE |\n",
    "| S06   | Aggregate+           | 68           | 32              | COMPLETE |\n",
    "| S08   | Coalesce             | 32           | 32              | COMPLETE |\n",
    "| S09   | Join+                | 1670         | 10              | COMPLETE |\n",
    "| S0A   | Output               | 10           | 10              | COMPLETE |\n",
    "\n",
    "#### Query Plan: q2_time\n",
    "\n",
    "| Step  | Descripción         | Records Read | Records Written | Status   |\n",
    "|-------|----------------------|--------------|-----------------|----------|\n",
    "| S00   | Input                | 117407       | 361             | COMPLETE |\n",
    "| S01   | Sort+                | 361          | 10              | COMPLETE |\n",
    "| S02   | Output               | 10           | 10              | COMPLETE |\n",
    "\n",
    "### Comparación Query Plan enfoque memory vs time\n",
    "\n",
    "| Step | q2_memory (Q2_MEMORY_QUERY)                                   | q2_time (Q2_TIME_QUERY)                                    |\n",
    "|------|---------------------------------------------------------------|-----------------------------------------------------------|\n",
    "| S00  | **Input**: Records read: 117407, Records written: 361, Status: COMPLETE | **Input**: Records read: 117407, Records written: 361, Status: COMPLETE |\n",
    "| S01  | **Aggregate**: Records read: 361, Records written: 70, Status: COMPLETE | **Sort+**: Records read: 361, Records written: 10, Status: COMPLETE |\n",
    "| S02  | **Aggregate+**: Records read: 70, Records written: 1, Status: COMPLETE | **Output**: Records read: 10, Records written: 10, Status: COMPLETE |\n",
    "| S03  | **Aggregate**: Records read: 1, Records written: 1, Status: COMPLETE |                                                           |\n",
    "| S04  | **Sort+**: Records read: 71, Records written: 70, Status: COMPLETE |                                                           |\n",
    "| S05  | **Aggregate**: Records read: 70, Records written: 68, Status: COMPLETE |                                                           |\n",
    "| S06  | **Aggregate+**: Records read: 68, Records written: 32, Status: COMPLETE |                                                           |\n",
    "| S08  | **Coalesce**: Records read: 32, Records written: 32, Status: COMPLETE |                                                           |\n",
    "| S09  | **Join+**: Records read: 1670, Records written: 10, Status: COMPLETE |                                                           |\n",
    "| S0A  | **Output**: Records read: 10, Records written: 10, Status: COMPLETE |                                                           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de los resultados anteriores se evidencia de que q2_time tiene una mejora con respecto al tiempo de ejecución versus su contraparte q2_memory en casi un 50%.\n",
    "Esto se evidencia en los query plan de cada enfoque. Mientras que q2_memory tiene 10 pasos, q2_time tiene 3 (considerando output). En donde q2_memory va leyendo de a poco la data en cada paso, optimizando el uso de memoria durante la ejecución. Mientras que la version enfocada en el tiempo de ejecución se centra en obtener todo rápido (algo como un algoritmo voraz) realiza toda la ingesta y procesa todo sin depender de steps intermedios. La desventaja de esto es que el consumo de memoria y cpu es elevado. Sin embargo, la medición muestra que la máquina optimizo el uso de CPU y paso menos tiempo en CPU (dado la cantidad de steps). Cabe destacar que el enfoque de q2_time es leer todo en el primer step, luego step sort y luego output. El overhead inicial de consulta es alto en termino de CPU, memoria y disco. Sin embargo, paso mucho menos tiempo en CPU por el tamaño del dataset. \n",
    "El dataset al ser pequeño logro realizar todo de forma rápida, logrando un uso minimo de tiempo CPU (813 ms) vs su contraparte memory (28137 ms). Sin embargo, tal como se ve en la comparación de query plan, si el dataset hubiera sido del orden de los GBs el overhead y uso de CPU se dispararía dada la cantidad de steps (pocos) en q2_time, tradeoffs, lo que gano en procesamiento no es escalable a un dataset más grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Top 10 histórico de usuarios (username) más influyentes (más mencionados)\n",
    "\n",
    "## Q3: SQL MEMORY\n",
    "\n",
    "```python \n",
    "Q3_MEMORY_QUERY: str = \"\"\"\n",
    "WITH filtered_mentions AS (\n",
    "  SELECT mentionedUser.username\n",
    "  FROM `{file_path}`,\n",
    "       UNNEST(mentionedUsers) AS mentionedUser\n",
    "  WHERE mentionedUsers IS NOT NULL\n",
    "    AND mentionedUser.username IS NOT NULL\n",
    ")\n",
    "SELECT username, COUNT(*) AS mention_count\n",
    "FROM filtered_mentions\n",
    "GROUP BY username\n",
    "ORDER BY mention_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 18:30:29,041 - INFO - Starting: q3_memory\n",
      "2024-07-05 18:30:29,042 - INFO - Ejecutando query\n",
      "2024-07-05 18:30:30,504 - INFO - Tiempo de ejecución de la query (lado del cliente, python, hasta recibir la respuesta): 1.46 segundos\n",
      "2024-07-05 18:30:30,505 - INFO - Se uso query cache?: False\n",
      "2024-07-05 18:30:30,506 - INFO - Bigquery Job Detail:\n",
      "2024-07-05 18:30:30,506 - INFO - Job ID: 6f194a9e-0755-4bc2-a0cb-4ef2d8b6da0c\n",
      "2024-07-05 18:30:30,507 - INFO - Job Status: DONE\n",
      "2024-07-05 18:30:30,507 - INFO - Tiempo inicio en máquina GCP: 2024-07-05 22:30:29.521000+00:00\n",
      "2024-07-05 18:30:30,508 - INFO - Tiempo fin en máquina GCP: 2024-07-05 22:30:29.943000+00:00\n",
      "2024-07-05 18:30:30,508 - INFO - Tiempo de ejecución total en servidor GCP: 0.422 segundos\n",
      "2024-07-05 18:30:30,509 - INFO - Tiempo de ejecución total en cliente (python): 1.46 segundos\n",
      "2024-07-05 18:30:30,509 - INFO - Delta de tiempo (costo de red, serialización, SO, etc.): 1.04 segundos\n",
      "2024-07-05 18:30:30,510 - INFO - Bytes procesados: 7504281\n",
      "2024-07-05 18:30:30,510 - INFO - Bytes facturados: 10485760\n",
      "2024-07-05 18:30:30,510 - INFO - Slot machine miliseconds: 605\n",
      "2024-07-05 18:30:30,511 - INFO - Uso estimado de memoria en base a total bytes procesados: 7.16 MB\n",
      "2024-07-05 18:30:30,511 - INFO - Query Plan:\n",
      "2024-07-05 18:30:30,512 - INFO -   Step S00: Input:\n",
      "2024-07-05 18:30:30,512 - INFO -     - Records read: 117407\n",
      "2024-07-05 18:30:30,513 - INFO -     - Records written: 31260\n",
      "2024-07-05 18:30:30,513 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 18:30:30,513 - INFO -   Step S01: Sort+:\n",
      "2024-07-05 18:30:30,514 - INFO -     - Records read: 31260\n",
      "2024-07-05 18:30:30,514 - INFO -     - Records written: 10\n",
      "2024-07-05 18:30:30,514 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 18:30:30,515 - INFO -   Step S02: Output:\n",
      "2024-07-05 18:30:30,515 - INFO -     - Records read: 10\n",
      "2024-07-05 18:30:30,515 - INFO -     - Records written: 10\n",
      "2024-07-05 18:30:30,516 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 18:30:30,852 - INFO - Sucessful finish: q3_memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "# Ejecución q3_memory\n",
    "import q3_memory\n",
    "import importlib\n",
    "importlib.reload(q3_memory)\n",
    "result = q3_memory.q3_memory(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pasos q3_memory:\n",
    "\n",
    "1. Filtrar menciones de usuarios:\n",
    "    1.1 Crea CTE: filtered_mentions:\n",
    "        1.1.1 Selecciona los nombres de usuario mencionados (mentionedUser.username)\n",
    "        1.1.2 Descompone los arrays de usuarios mencionados en elementos individuales usando UNNEST.\n",
    "        1.1.3 Filtra los resultados para asegurarse de que \"mentionedUsers\" no sea nulo y que \"mentionedUser.username\" no sea nulo. Con esto ahorramos memoria en siguientes pasos del query plan.\n",
    "2. Contar las menciones agrupando los nombres de usuario por el campo username.\n",
    "3. Selecciona el top10 usuarios ordenando por mention_count descendente y limitando los resultados.\n",
    "\n",
    "\n",
    "#### Análisis q3_memory\n",
    "\n",
    "* **uso de memoria**: bajo. \"unnest\" ayuda a trabajar los datos en pequeñas listas reduciendo la cantidad de datos en memoria por cada query step. A esto se le suma la CTE \"filtered_mentions\" que permite filtrar y reducir el espacio de búsquedad antes de las agregaciones, minimizando el uso posterior de memoria.\n",
    "* **tiempo de ejecución**: alto. Las operaciones de filtrado son caras pero se realizan al principio.\n",
    "* **uso de CPU**: intermedio. Las operaciones de \"unnest\" es cpu expensive. Lo cuál puede aumentar los costos de billing en GCP.\n",
    "* **uso de disco**: bajo. Idem que enfoque anterior. La extracción de usernames y counts se realizan en memoria, por lo que el acceso a disco es bajo (generalmente el sistema operativo o scheduler de la BD almacenando páginas).\n",
    "\n",
    "Este enfoque es eficiente en memoria debido a: i) uso de \"UNNEST\" que descompone el array en elementos individuales, permitiendo procesar en siguientes pasos del query plan menos cantidad de data de forma progresiva, ii) el filtro inicial realizado por la CTE \"filtered_mentiosn\" que filtra y reduce el conjunto de datos antes de la agregación y iii) group y count se realiza después de haber reducido el total de datos.\n",
    "\n",
    "\n",
    "#### Posible mejora (q3_memory):\n",
    "* Indice en los campos de usuarios mencionados o materialized views para almacenar precomputacion de consultas frequentes. Ahora bien, el uso de CPU con indices se ve disminuido pero aumenta costo en disco (tradeoffs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: SQL TIME\n",
    "\n",
    "```python \n",
    "Q3_TIME_QUERY: str = r\"\"\"\n",
    "WITH mention_counts AS (\n",
    "  SELECT mentionedUser.username, COUNT(*) AS mention_count\n",
    "  FROM `{file_path}`,\n",
    "       UNNEST(mentionedUsers) AS mentionedUser\n",
    "  WHERE mentionedUsers IS NOT NULL\n",
    "    AND mentionedUser.username IS NOT NULL\n",
    "  GROUP BY mentionedUser.username\n",
    ")\n",
    "SELECT username, mention_count\n",
    "FROM mention_counts\n",
    "ORDER BY mention_count DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 18:57:38,050 - INFO - Starting: q3_time\n",
      "2024-07-05 18:57:38,051 - INFO - Ejecutando query\n",
      "2024-07-05 18:57:39,561 - INFO - Tiempo de ejecución de la query (lado del cliente, python, hasta recibir la respuesta): 1.51 segundos\n",
      "2024-07-05 18:57:39,562 - INFO - Se uso query cache?: False\n",
      "2024-07-05 18:57:39,563 - INFO - Bigquery Job Detail:\n",
      "2024-07-05 18:57:39,564 - INFO - Job ID: aa238f0b-6963-4b96-a512-400b959ca8f5\n",
      "2024-07-05 18:57:39,564 - INFO - Job Status: DONE\n",
      "2024-07-05 18:57:39,565 - INFO - Tiempo inicio en máquina GCP: 2024-07-05 22:57:38.762000+00:00\n",
      "2024-07-05 18:57:39,566 - INFO - Tiempo fin en máquina GCP: 2024-07-05 22:57:39.147000+00:00\n",
      "2024-07-05 18:57:39,566 - INFO - Tiempo de ejecución total en servidor GCP: 0.385 segundos\n",
      "2024-07-05 18:57:39,566 - INFO - Tiempo de ejecución total en cliente (python): 1.51 segundos\n",
      "2024-07-05 18:57:39,567 - INFO - Delta de tiempo (costo de red, serialización, SO, etc.): 1.12 segundos\n",
      "2024-07-05 18:57:39,568 - INFO - Bytes procesados: 7504281\n",
      "2024-07-05 18:57:39,568 - INFO - Bytes facturados: 10485760\n",
      "2024-07-05 18:57:39,569 - INFO - Slot machine miliseconds: 562\n",
      "2024-07-05 18:57:39,569 - INFO - Uso estimado de memoria en base a total bytes procesados: 7.16 MB\n",
      "2024-07-05 18:57:39,569 - INFO - Query Plan:\n",
      "2024-07-05 18:57:39,570 - INFO -   Step S00: Input:\n",
      "2024-07-05 18:57:39,570 - INFO -     - Records read: 117407\n",
      "2024-07-05 18:57:39,571 - INFO -     - Records written: 31260\n",
      "2024-07-05 18:57:39,571 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 18:57:39,571 - INFO -   Step S01: Sort+:\n",
      "2024-07-05 18:57:39,572 - INFO -     - Records read: 31260\n",
      "2024-07-05 18:57:39,572 - INFO -     - Records written: 10\n",
      "2024-07-05 18:57:39,573 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 18:57:39,573 - INFO -   Step S02: Output:\n",
      "2024-07-05 18:57:39,573 - INFO -     - Records read: 10\n",
      "2024-07-05 18:57:39,574 - INFO -     - Records written: 10\n",
      "2024-07-05 18:57:39,574 - INFO -     - Status: COMPLETE\n",
      "2024-07-05 18:57:39,920 - INFO - Successful finish: q3_time\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('narendramodi', 2265), ('Kisanektamorcha', 1840), ('RakeshTikaitBKU', 1644), ('PMOIndia', 1427), ('RahulGandhi', 1146), ('GretaThunberg', 1048), ('RaviSinghKA', 1019), ('rihanna', 986), ('UNHumanRights', 962), ('meenaharris', 926)]\n"
     ]
    }
   ],
   "source": [
    "# q3_time \n",
    "import importlib\n",
    "import q3_time\n",
    "importlib.reload(q3_time)\n",
    "result = q3_time.q3_time(file_path)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pasos q3_time:\n",
    "\n",
    "A diferencia del enfoque anterior, q3_time se enfoca en realizar la extracción, count y agregación en una sola ejecución/pasada. No utiliza múltiples etapas, disminuyendo la cantidad de steps en el query plan. No hay múltiples etapas de filtrado y agregación reduciendo el tiempo total de procesamiento. Mientras que q3_memory realiza un filtrado inicial antes de la agregación, introduciendo una etapa adicional de procesamiento (más tiempo de procesamiento pero menor uso de memoria).\n",
    "\n",
    "Por otro lado, q3_time realiza la agrupación y count de forma inmediata en la misma CTE \"mention_counts\", optimizando el uso de CPU y latencia, que conlleva menor tiempo de procesamiento. Mientras que q3_memory primero filtra los datos en una CTE (\"filtered_mentions\") y luego realiza la agregación, castigando el tiempo de CPU pero disminuyendo el uso de memoria.\n",
    "\n",
    "Por último, q3_time realiza todas las operaciones de filter, descomposición, agrupación y count en una sola ejecución/pasada, minimizando la cantidad de operaciones y reduciendo el tiempo de ejecución. Mientras que, q3_memory añade un paso adicional para filter y agrupar introduciendo más operaciones.\n",
    "\n",
    "\n",
    "#### Análisis q3_time\n",
    "\n",
    "* **uso de memoria**: intermedio. \"unnest\" explota los datos en este caso, pero no aumenta tanto debido a que se realiza en una sóla etapa de agregaciones y filtrados.\n",
    "* **tiempo de ejecución**: bajo en comparación con enfoque anterior. Las operaciones se realizan en una sola ejecución, evitando pasos/instrucciones adicionales. Todo lo realiza en una sola CTE.\n",
    "* **uso de CPU**: alto. Las operaciones de \"unnest\" es cpu expensive al igual que la agregación.\n",
    "* **uso de disco**: bajo. Mismo argumento de enfoques pasados.\n",
    "\n",
    "\n",
    "#### Posible mejora (q3_time):\n",
    "* El formato de almacenamiento podría cambiar a Parquet o Avro para el almacenamiento, luego al ser comprimidos mejora el tiempo de lectura dependiendo de la consulta, por ejemplo, si es más columnar o row oriented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla Comparativa de Mediciones Q3\n",
    "\n",
    "### Configuración del job Q3\n",
    "\n",
    "| Parámetros                                      | q3_memory | q3_time |\n",
    "|------------------------------------------------|-----------|---------|\n",
    "| QUERY_CACHE                                    | False     | False   |\n",
    "| QUERY_PRIORITY                                 | INTERACTIVE     | INTERACTIVE|\n",
    "\n",
    "### Resultados mediciones\n",
    "\n",
    "| Medición                                      | q3_memory | q3_time |\n",
    "|-----------------------------------------------|-----------|---------|\n",
    "| Tiempo ejecución en cliente                   | 1.46 s    | 1.51 s  |\n",
    "| Tiempo ejecución en GCP (server)              | 0.422 s   | 0.385 s |\n",
    "| Delta tiempo (client-server)                  | 1.04 s    | 1.12 s  |\n",
    "| Memoria usada por cliente python              | NA   | NA |\n",
    "| CPU usado por cliente python                  | NA        | NA      |\n",
    "| Bytes Procesados en GCP (server)              | 7.16 MB   | 7.16 MB |\n",
    "| Bytes Facturados en GCP (server)              | 10.49 MB  | 10.49 MB|\n",
    "| Slot Machine tiempo usado (GCP CPU Time) (ms) | 605 ms    | 562 ms  |\n",
    "| BigQuery Total steps en QueryPlan             | 3         | 3       |\n",
    "\n",
    "### Resultados Query Plan\n",
    "\n",
    "#### Query Plan: q3_memory\n",
    "\n",
    "| Step  | Descripción         | Records Read | Records Written | Status   |\n",
    "|-------|----------------------|--------------|-----------------|----------|\n",
    "| S00   | Input                | 117407       | 31260           | COMPLETE |\n",
    "| S01   | Sort+                | 31260        | 10              | COMPLETE |\n",
    "| S02   | Output               | 10           | 10              | COMPLETE |\n",
    "\n",
    "#### Query Plan: q3_time\n",
    "\n",
    "| Step  | Descripción         | Records Read | Records Written | Status   |\n",
    "|-------|----------------------|--------------|-----------------|----------|\n",
    "| S00   | Input                | 117407       | 31260           | COMPLETE |\n",
    "| S01   | Sort+                | 31260        | 10              | COMPLETE |\n",
    "| S02   | Output               | 10           | 10              | COMPLETE |\n",
    "\n",
    "### Comparación Query Plan enfoque memory vs time\n",
    "\n",
    "| Step | q3_memory (Q3_MEMORY_QUERY)                                   | q3_time (Q3_TIME_QUERY)                                    |\n",
    "|------|---------------------------------------------------------------|-----------------------------------------------------------|\n",
    "| S00  | **Input**: Records read: 117407, Records written: 31260, Status: COMPLETE | **Input**: Records read: 117407, Records written: 31260, Status: COMPLETE |\n",
    "| S01  | **Sort+**: Records read: 31260, Records written: 10, Status: COMPLETE | **Sort+**: Records read: 31260, Records written: 10, Status: COMPLETE |\n",
    "| S02  | **Output**: Records read: 10, Records written: 10, Status: COMPLETE | **Output**: Records read: 10, Records written: 10, Status: COMPLETE |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acá la diferencia entre un enfoque centrado en memory vs time es minima. \n",
    "Las razones de esto pueden ser:\n",
    "1. La consulta q3_memory ya esta optimizada en terminos de uso de CPU y latencia. Por lo que no queda tanto margen de mejora.\n",
    "2. OPORTUNIDAD DE MEJORA: los steps estan realizando las mismas operaciones de i/o por lo que la mejora no es explicado por uso de memoria y/o disco. Posiblemente la CPU o algún core paso menos tiempo (minimo) procesando alguna instrucción de las usadas en la consulta.\n",
    "3. Otra explicación puede ser que la diferencia es minima para este dataset pero a medida que el dataset crezca se puede evidenciar una brecha más alta con respecto a la medición de tiempo de ejecución y slot machine (CPU time usado).\n",
    "4. La única diferencia real es que el procesamiento bajo 40 ms con respecto a su contraparte q3_memory. No puedo atribuir que se deba a la optimización de la consulta sin realizar pruebas en profundidad y un análisis estadistico más pronfundo. Puede que simplemente sea el sistema operativo que mantuvo en cache o memoria ciertas páginas (pages) de rows de la BD como cache para acelerar futuras consultas (posible explicación es por el sistema operativo)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
